---
title: "fmm.proj1.Froehner"
author: "Cosima Froehner"
date: "2022-12-04"
output:
  pdf_document: default
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
getwd()
messungen <- read.csv("messungen.csv")
library("matlib")
library("pracma")
library("microbenchmark")
library("ggplot2")
```

# Noisy Data

Ein Physiker hat ein Experiment durchgeführt, um die Auswirkungen eines
elektromagnetischen Impulses zu erforschen. Er hatte dabei eine schöne,
glatte Kurve erwartet. Allerdings stellt sich das Messgerät als ungenau
heraus und die tatsächlichen Messungen sind stark verrauscht. Der
vorliegende Report beschäftigt sich nun mit dem Korrigieren des
Messfehlers und der Rekonstruktion einer glatten Kurve als Näherung für
die unverrauschten Messdaten. Dazu soll im Folgenden zunächst die
Kondition des Problems analysiert werden. Im Anschluss wird zur Lösung
des Problems ein Algorithmus geschrieben und implementiert, dessen
Laufzeit-Komplexität hergeleitet und abschließend mit einem weniger
stabilen Algorithmus für das Problem verglichen.

Die verrauschten Messdaten des Physikers sind im Datensatz `messungen`
mit den Spalten t und y enthalten und sehen folgendermaßen aus:

```{r}
plot(messungen$t, messungen$y, xlab = "Messzeitpunkte t", ylab = "Messungen y", main = "Verrauschte Daten")
```

# Modell zur Rekonstruktion

Wir stellen zunächst ein passendes mathematisches Modell auf, mit dem
wir die Daten $(Y_i, t_i)^n_{i=1}$ beschreiben:

$\mathbf{Y_i} = f(\mathbf{t_i}) + \mathbf{\epsilon_i}$

wobei f die zu rekonstruierende, glatte Funktion und
$\epsilon_1, … \epsilon_n$ Messfehler aufgrund des Messgeräts sind. Die
Messzeitpunkte $\mathbf{t} = t_1, ..., t_n$ sind die tatsächlichen
Erhebungszeitpunkte des Physikers, die wird als gegeben betrachten.

Für die Rekonstruktion treffen wir folgende Annahmen:

-   für beliebige Zeitpunkte $t^´_1, ..., t^´_m$ aus dem
    Erhebungsintervall [0,5] gilt
    $f(t^´) \sim \mathcal{}N(0, K_{t^´,t^´})$, wobei $K_{t^´, t^´}$ die
    Kovarianzmatrix - genauer Kernelmatrix[^1] - ist. Für alle
    $\mathbf{t} \in \mathbb{R^{n}}$ und $\mathbf{s} \in \mathbb{R^{m}}$
    werden Abstände mit der Gauß-Kernfunktion modelliert:

    ???

-   die Fehler $\mathbf{\epsilon_{1}}, ... \epsilon_{n}$ sind
    unabhängig, identisch $\mathcal{N}(0,\,\sigma^{2})$ verteilt und
    unabhängig von $f(\mathbf{t)}$.

    Die Modellvarianz $\gamma$ und die Fehlervarianz $\sigma^2$ können
    wir frei wählen.

[^1]: Die Kernelmatrix enthält die modellierten Kovarianzen zwischen
    jedem Paar von Messzeitpunkten. Zur Modellierung dieser Kovarianzen
    verwenden wir die Gaussche Kernfunktion $\exp(\frac{-s^2}{\gamma})$,
    die uns im Vergleich zu anderen Kernfunktionen eine möglichst glatte
    Funktion liefert. [[Gaussian processes (1/3) - From scratch
    (peterroelants.github.io)](https://peterroelants.github.io/posts/gaussian-process-tutorial/)]

Daraus folgt

\
$f(t^´| \mathbf{Y}=\mathbf{y}) \sim \mathcal{N(\mu_{2|1}, \Sigma_{2|1})}$

,wobei

$\mathbf\mu_{2|1} = \Sigma_{2,1}\Sigma_{1,1}^{-1}\mathbf{y}$

und
$\Sigma_{2|1} = \Sigma_{2,2}-\Sigma_{2,1}\Sigma_{1,1}^{-1}\Sigma_{1,2}$.

dass die Messungen $\mathbf{Y}$ aus den tatsächlich erhobenen Daten
ergänzt um $f(t^´)$ ebenfalls $\sim \mathcal{N(0,\Sigma)}$ , wobei ...

# Konditionierung des Problems

Um besser zu verstehen, wie stark unsere Lösung $mu_{2|1}$ von der
Störung der Eingangsdaten abhängt, betrachten wir zunächst die
Konditionierung der Teilprobleme.

Sei $(\mu_{2|1}, \Sigma_{2,1}\Sigma_{1,1},\mathbf{y})$ ein Problem mit
Lösung $\mu_{2|1} = \Sigma_{2,1}\Sigma_{1,1}^{-1}{y}$, wobei
$\Sigma_{2,1} \in \mathbb{R^{{n\times}n}}$,
$\Sigma_{1,1} \in \mathbb{R^{n{\times}n}}$ und
$\mathbf{y} \in \mathbb{R^{n}}$ mit $\Sigma_{1,1}$ invertierbar. Sei
außerdem $|\cdot|$ eine zulässige Norm.

Wir definieren die folgenden drei Teilprobleme:

$\mu_{2|1}(\Sigma_{2,1}) = \Sigma_{2,1}\Sigma_{1,1}^{-1}{y}$ (1)

$\mu_{2|1}(\Sigma_{1,1}) = \Sigma_{2,1}\Sigma_{1,1}^{-1}{y}$ (2)

$\mu_{2|1}(\mathbf{y}) = \Sigma_{2,1}\Sigma_{1,1}^{-1}{y}$ (3).

Dann gilt:

$\frac{|{\tilde\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y} - {\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} = \frac{|({\tilde\Sigma_{2,1}} - {\Sigma_{2,1}})\Sigma_{1,1}^{-1}\mathbf{y}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} \leq^{*1} \frac{|{\tilde\Sigma_{2,1}} - {\Sigma_{2,1}}||\Sigma_{1,1}^{-1}\mathbf{y}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} = \frac{|{\Sigma_{2,1}}||\Sigma_{1,1}^{-1}\mathbf{y}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} \frac{|{\tilde\Sigma_{2,1}} - {\Sigma_{2,1}}|}{|{\Sigma_{2,1}}|} = \kappa_1 \frac{|{\tilde\Sigma_{2,1}} - {\Sigma_{2,1}}|}{|{\Sigma_{2,1}}|}$
mit $\kappa_1$ als Konditionszahl für (1)

$\frac{|{\Sigma_{2,1}}\tilde\Sigma_{1,1}^{-1}\mathbf{y} - \Sigma_{2,1}\Sigma_{1,1}^{-1}\mathbf{y}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} \leq \frac{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}(\tilde{\mathbf{y}} - \mathbf{y})|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} \leq^{*1} \frac{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}||\tilde{\mathbf{y}} - \mathbf{y}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} = \frac{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}||\mathbf{y}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} \frac{|{\tilde{y}} - {y}|}{|{y}|} = \kappa_3 \frac{|{\tilde{y}} - {y}|}{|{y}|}$
mit $\kappa_3$ als Konditionszahl für (3)

$\frac{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y} - {\Sigma_{2,1}}\Sigma_{1,1}^{-1}\tilde{\mathbf{y}}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} \leq^{*1} \frac{|{\Sigma_{2,1}}||\tilde\Sigma_{1,1}^{-1}-\Sigma_{1,1}^{-1}||\mathbf{y}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} \leq^{*2} \frac{|\Sigma_{1,1}^{-1}|^{2}|{\Sigma_{2,1}}||\mathbf{y}||\Sigma_{1,1}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} \frac{|\tilde\Sigma_{1,1}^{-1} - \Sigma_{1,1}^{-1}|}{|\Sigma_{1,1}|} \leq |\Sigma_{1,1}||\Sigma_{1,1}^{-1}| \frac{|\Sigma_{1,1}^{-1}||{\Sigma_{2,1}}||\mathbf{y}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} \frac{\tilde\Sigma_{1,1}^{-1} - \Sigma_{1,1}^{-1}}{\Sigma_{1,1}^{-1}} = \kappa_2\frac{\tilde\Sigma_{1,1}^{-1} - \Sigma_{1,1}^{-1}}{\Sigma_{1,1}^{-1}}$
mit $\kappa_2$ Konditionierungszahl für (2) als Produkt der
Konditionszahl $|\Sigma_{1,1}||\Sigma_{1,1}^{-1}|$ für die Berechnung
der Inversen von $\Sigma_{1,1}$ und der Konditionszahl
$\frac{|\Sigma_{1,1}^{-1}||{\Sigma_{2,1}}||\mathbf{y}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|}$
für die Matrixmultiplikation.

Die allgemeine Schranke für den (relativen) Outputfehler bezüglich aller
relativen Inputfehler ergibt sich aus der Summe der einzelnen Produkte
von Konditionszahl $\kappa_{\{1,2,3\}}$ und jeweiligem Fehler. Also:

$*^1$ Submultiplikativität

$*^2$ Folgt aus Aufgabe 2.1c) der Übung inkl. Beweis zur Stetigkeit der
Matrixinvertierung in der Lösung

Theorem 1.1

Sei $A \in \mathbb{R^{n{\times}n}}$ invertierbar. Sei außerdem $|\cdot|$
eine zulässige Norm. Dann gilt für die (relative) Kondition:

$\kappa(A) = ||A||||A^{-1}||$, wobei die Kondition von der gewählten
Matrixnorm abhängt.

# Aufgabe 3 - Einfluss der Hyperparameter (Best - und Worst Case)

Um den Einfluss der Hyperparameter $\gamma$ und $\sigma$ näher zu
untersuchen, visualisieren wir zunächst den Einfluss der Hyperparameter
auf die Konditionszahl von $\Sigma_{1,1}$ für festes $\sigma = 1$ bzw.
festes $\gamma = 1$.

```{r}
sigma.quadrat <- 1

kappa.fun1 <- function(gamma) {
  Kernfun <- function(s) {
    exp(-(s ^ 2) / gamma)
  }
  matK <-
    function(t, s) {
      Kernfun(abs(
        matrix(t, nrow = length(t), ncol = length(s)) - matrix(
          s,
          nrow = length(t),
          ncol = length(s),
          byrow = TRUE
        )
      ))
    }
  Sigma1.1 <-
    matK(messungen$t, messungen$t) + sigma.quadrat * diag(length(messungen$t))
  return(max(svd(Sigma1.1)$d) / min(svd(Sigma1.1)$d))
}

plot(gamma, sapply(gamma, kappa.fun1), type = "l", xlab = "gamma", ylab = "Konditionszahl")



gamma <- seq(0.1, 10, by = .1)

```

```{r}
gamma <- 1

kappa.fun2 <- function(sigma.quadrat) {
  Kernfun <- function(s) {
    exp(-(s ^ 2) / gamma)
  }
  matK <-
    function(t, s) {
      Kernfun(abs(
        matrix(t, nrow = length(t), ncol = length(s)) - matrix(
          s,
          nrow = length(t),
          ncol = length(s),
          byrow = TRUE
        )
      ))
    }
  Sigma1.1 <-
    matK(messungen$t, messungen$t) + sigma.quadrat * diag(length(messungen$t))
  return(max(svd(Sigma1.1)$d) / min(svd(Sigma1.1)$d))
}

plot(sigma.quadrat, sapply(sigma.quadrat, kappa.fun2), type = "l", xlab = "sigma", ylab = "Konditionszahl")


sigma.quadrat <- seq(0.1, 10, by = .1)
```

Da die Konditionszahl des Gesamtproblems von der Kondition der
Kernmatrizen für $\Sigma_{1,1}$ und $\Sigma_{2,1}$ abhängt und diese
wiederum von der Kernfunktion $\exp(\frac{-s^2}{\gamma})$ abhängig sind,
betrachten wir zunächst den Einfluss von $\gamma$ auf die Konditionszahl
der Kernfunktion. Diese ergibt sich aus der Ableitung der Kernfunktion
nach $\gamma$:

$\kappa_{abs} = |\frac{\delta exp(\frac{-s^2}{\gamma})}{\delta\gamma}| = |exp(\frac{-s^2}{\gamma})\frac{s^2}{\gamma^2}| = exp(\frac{-s^2}{\gamma})\frac{s^2}{\gamma^2}$

$\rightarrow \kappa_{rel} = \frac{|\gamma|}{|exp(\frac{-s^2}{\gamma})|}exp(\frac{-s^2}{\gamma})\frac{s^2}{\gamma^2} = \frac{\gamma s^2}{\gamma^2} = \frac{s^2}{\gamma}$

Folglich ist für Werte von $\gamma \approx 0$ die Konditionszahl der
Kernfunktion groß und die das Problem damit schlecht konditioniert. Für
$\gamma \geq s^2$ ist die Konditionszahl $\leq 1$ und das Problem
folglich gut konditioniert.

Da der Hyperparameter $\sigma$ nur zur Berechnung von $\Sigma_{1,1}$
verwendet wird, betrachten wir uns die Konditionszahl dieser Matrix
mittels maximalem Singulärwert $\sigma_{max}$ und minimalem Singulärwert
$\sigma_{min}$ :

$\kappa_{\Sigma_{1,1}} = \frac{\sigma_{max}}{\sigma_{min}} = \frac{\Sigma_{1,1}}{\sigma_{min}} =^{*} \frac{K_{t,t} + \sigma^2I}{\sigma^2} > 1$

Folglich ist das Problem für $\sigma^2 \approx 0$ schlecht und für
$K_{t,t} \approx 0$ gut konditioniert.

$^* \sigma_{min} = \sigma^2$ folgt aus Beobachtung.

Dieser Algorithmus ist schlecht, weil er die Inverse einer Matrix
verwendet. Ein Algorithmus, der eine Inverse verwendet, ist im
Allgemeinen instabiler als ein Algorithmus ohne Inverse, da die Inverse
nicht immer eindeutig ist. Auch wenn es nur eine Inverse gibt, kann der
Algorithmus immernoch anfälliger sein, weil die Berechnung einer
Inversen i.d.R. numerisch anspruchsvoller ist als die Berechnung anderer
Funktionen. Die Verwendung einer Inversen kann daher dazu führen, dass
kleine numerische Fehler, die während der Berechnungen auftreten, sich
verstärken und zu größeren Fehlern in den endgültigen Ergebnissen
führen. Dies kann insbesondere dann ein Problem sein, wenn der
Algorithmus mit großen Datenmengen arbeitet oder wenn er mit sehr
kleinen Zahlen oder sehr großen Zahlen arbeitet, die schnell an
Genauigkeit verlieren können. Insgesamt ist es daher im Allgemeinen
ratsam, Algorithmen zu vermeiden, die eine Inverse verwenden, wenn es
möglich ist. Im Folgenden werden wir uns deshalb Zerlegungen von
Sigma1.1 zunutze machen, um y ohne Inverse zu berechnen.

Außerdem zu erwähnen: alg1 verwendet eine hohe Anzahl an Operationen,
ist also aufwändiger und hat eine entsprechend längere Laufzeit.
Zusätzlich werden mehrere Matrizen zwischengespeichert. Dadurch benötigt
alg1 mehr Speicherplatz, was die Laufzeit weiter erhöht.

# Stabiler und effizienter Algorithmus

Die Matrix Sigma1.1 ist symmetrisch. Folglich können wir sie orthogonal
diagonalisieren und damit die Berechnung der Inversen vermeiden, da für
orthogonale Matrizen gilt: solve(A) = t(A) und die Inverse einer
Diagonalmatrix sich einfach ergibt, indem man die Kehrbrüche auf der
Diagonalen berechnet.

```{r}
# Kernfun: 
# Kernfun ist die Kernelfunktion, die als Prior dient und die Kovarianz für jede einzelne Kombination von Messzeitpunkten modelliert 
# Input: absolute Differenz s zwischen zwei Messzeitpunkten
# Output: modellierte Kovarianz
Kernfun <- function(s) {
  exp(-(s ^ 2) / gamma)
}

# matK: 
# matK ist die Kernmatrix
# Input: Vektor t der Länge n, der Messzeitpunkte enthält, Vektor s mit Länge m, der Messzeitpunkte enthält
# Output: eine Matrix n x m mit den modellierten Kovarianzen aus der Kernelfunktion als Einträge
matK <-
  function(t, s) {
    Kernfun(abs(
      matrix(t, nrow = length(t), ncol = length(s)) - matrix(
        s,
        nrow = length(t),
        ncol = length(s),
        byrow = TRUE
      )
    ))
  }
```

```{r}
#Definiere freie (Hyper-)Parameter
t.Strich <- seq(from = 0, to = 5, l = 100)
gamma <- 1
sigma.quadrat <- 1
```

```{r}
# alg_OD
# Berechnet mü2.1 mithilfe orthogonaler Diagonalisierung
# Input: 
## t.Strich: Vektor der Länge m mit Messzeitpunkten
## sigma.quadrat > 0 mit default = 1
## gamma > 0 mit default = 1
# Output:
## Vektor mü2.1 der Länge m

alg_OD <- function(t.Strich, sigma = 1, gamma = 1){
  Sigma1.1 <-
    matK(messungen$t, messungen$t) + sigma.quadrat * diag(length(messungen$t))
  Sigma2.1 <- t(matK(messungen$t, t.Strich))
  P <- GramSchmidt(eigen(Sigma1.1)$vectors)  #Orthogonalisierung und Normalisierung mithilfe Gram Schmidt Algorithmus aus matlib package
  D.inv <- diag(1/eigen(Sigma1.1)$values)  
  y <- Sigma2.1 %*% t(P) %*% D.inv %*% P %*% messungen$y
  y
}
```

```{r}
# alg_CH
# berechnet mü2.1 mittels Cholesky Zerlegung
# Input: 
## t.Strich: Vektor der Länge m mit Messzeitpunkten
## sigma.quadrat > 0 mit default = 1
## gamma > 0 mit default = 1
# Output:
## Vektor mü2.1 der Länge m

algCH <- function(t.Strich, sigma = 1, gamma = 1){
  Sigma1.1 <-
    matK(messungen$t, messungen$t) + sigma.quadrat * diag(length(messungen$t))
  Sigma2.1 <- t(matK(messungen$t, t.Strich))
  L <- t(chol(Sigma1.1))
  B <- forwardsolve(L, messungen$y)
  Sigma2.1 %*% backsolve(t(L), B)
}
```

Bei $\Sigma_{1,1}$ handelt es sich um eine SPD-Matrix , da sie
symmetrisch und positiv definit ist.

```{r}
dim(Sigma1.1)  #n x n
all(Sigma1.1 == t(Sigma1.1))  #TRUE
all(svd(Sigma1.1)$D > 0)  #TRUE
```

Das erlaubt es uns die Cholesky-Zerlegung zu verwenden und
$\Sigma_{1,1}$ zu $\Sigma_{1,1} = LL^t$ zu zerlegen, wobei $L$ eine
untere Dreiecksmatrix ist.

Vorgehen!! Substit.

Mit der Cholesky-Zerlegung sparen wir uns Rechen- und Speicheraufwand,
der im Allgemeinen halb so groß ist wie bei der LU-Zerlegung.

-   forwardsolve
-   backsolve

# Aufgabe 6 - Schlechter Algorithmus

```{r}
# Funktion matK
# Berechnet die Kernmatrix 
# Input: 
  # t: Messzeitpunkte der erhobenen Daten mit default t aus der Tabelle Messungen
  # s: beliebige Messzeitpunkte
# Output: Rekonstruktion der unverrauschten Messwerte

Kernfun <- function(s) {
  exp(-(s ^ 2) / gamma)
}

matK <-
  function(t, s) {
    Kernfun(abs(
      matrix(t, nrow = length(t), ncol = length(s)) - matrix(
        s,
        nrow = length(t),
        ncol = length(s),
        byrow = TRUE
      )
    ))
  }

  
alg1 <- function(t.Strich,
                 gamma = 1,
                 sigma.quadrat = 1) {
  Kernfun <- function(s) {
    exp(-(s ^ 2) / gamma)
  }
  matK <-
    function(t, s) {
      Kernfun(abs(
        matrix(t, nrow = length(t), ncol = length(s)) - matrix(
          s,
          nrow = length(t),
          ncol = length(s),
          byrow = TRUE
        )
      ))
    }
  Sigma1.1 <-
    matK(messungen$t, messungen$t) + sigma.quadrat * diag(length(messungen$t))
  Sigma2.1 <- t(matK(messungen$t, t.Strich))
  y <- Sigma2.1 %*% solve(Sigma1.1) %*% messungen$y
  y
}
```

# Aufgabe 5 - Laufzeit-Komplexität

Wie verändert sich die Laufzeitkomplexität für die Längen 1, 10, 100,
1000 von t.Strich?

```{r}
t.Strich.1 <- seq(from = 0, to = 5, l = 1)

t.Strich.10 <- seq(from = 0, to = 5, l = 10)

t.Strich.100 <- seq(from = 0, to = 5, l = 100)

t.Strich.1000 <- seq(from = 0, to = 5, l = 1000)
```

Insgesamt beträgt die Laufzeit-Komplexität O(mn + n\^3).

Herleitung: - Sigma1.1: O(n\^2) - Sigma2.1: O(mn) - chol-Zerlegung:
O(n\^3) - Lösen LGS jeweils: O(n\^2) - Multiplikation Sigma2.1 und
Ergebnisvektor aus der Zerlegung: O(mn)

Im Allgemeinen beträgt die Laufzeitkomplexität einer Cholesky-Zerlegung
O(n\^3), was bedeutet, dass die Laufzeit quadratisch mit der Größe der
Matrix steigt. Das liegt daran, dass die Cholesky-Zerlegung auf dem
Algorithmus der Gauss-Elimination basiert, der selbst eine Laufzeit von
O(n\^3) hat.

```{r}
grid <- expand.grid(
  m = seq.int(10, 100000, length = 4),
  n = seq.int(10, 100000, length = 4)
)

times1 <- numeric(nrow(grid))
times2 <- numeric(nrow(grid))

for (k in 1:nrow(grid)) {
m <- grid[k, 1]
n <- grid[k, 2]
times1[k] <- median(microbenchmark(alg3(m, sigma = 1, gamma = 1), times = 10)$time)
times2[k] <- median(microbenchmark(alg1(m, sigma = 1, gamma = 1), times = 10)$time)
}
```

```{r}
cbind(grid, time = times1) |>
ggplot(aes(m, time, color = as.factor(n))) +
geom_line()
```

```{r}
cbind(grid, time = times2) |>
ggplot(aes(n, time, color = as.factor(m))) +
geom_line()
```

# Aufgabe 7 - Visualisierung

```{r}
x <- t.Strich
curve(alg3(x, sigma = 1, gamma = 0.0001), from = 0, to = 5, col = "red", ylim = c(0, max(messungen$y, alg3(t.Strich))), ylab = "y", xlab = "t.Strich")
lines(t.Strich, alg3(x, sigma = 1, gamma = 1), col = "green")
lines(t.Strich, alg1(x, sigma = 1, gamma = 0.0001), col = "blue")
points(messungen$t, messungen$y)
```

# Aufgabe 8 - Einfluss der Parameter und Numerisches Versagen

```{r}
# Define the function to compute MSE
mse <- function(gamma) {
  # Compute MSE for a given sigma value
  mean(messungen$y - alg3(messungen$t, sigma = 1, gamma))^2 + var(messungen$y - alg3(messungen$t, sigma = 1, gamma))^2
}

# Generate a sequence of sigma values
gammas <- seq(from = 0.0001, to = 100, by = 100)

# Compute the MSE for each sigma value
mse_values <- mapply(mse, gammas)

# Plot the MSE values using the curve function
plot(mse_values, xlim=c(-3, 100), ylim=c(-1, 120), col="red", xlab="Gamma", ylab="MSE", type = "l")
```

# Fazit

Kondition, Stabilität, Effizienzbetrachtung
