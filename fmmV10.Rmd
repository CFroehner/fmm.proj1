---
title: ""
author: "Cosima Froehner"
date: "2022-12-16"
output:
  pdf_document: default
  toc: true
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
getwd()
messungen <- read.csv("messungen.csv")
library("matlib")
library("pracma")
library("microbenchmark")
library("ggplot2")
```

# 1 Noisy Data

Ein Physiker hat ein Experiment durchgeführt, um die Auswirkungen eines
elektromagnetischen Impulses zu erforschen. Er hatte dabei eine schöne,
glatte Kurve erwartet. Allerdings stellt sich das Messgerät als ungenau
heraus und die tatsächlichen Messungen sind stark verrauscht. Der
vorliegende Report beschäftigt sich nun mit dem Korrigieren des
Messfehlers und der Rekonstruktion einer glatten Kurve als Näherung für
die unverrauschten Messdaten. Dazu soll im Folgenden zunächst die
Kondition des Problems analysiert werden. Im Anschluss wird zur Lösung
des Problems ein Algorithmus geschrieben und implementiert, dessen
Laufzeit-Komplexität hergeleitet und mit einem weniger stabilen
Algorithmus für das Problem verglichen wird. Abschließend werden die
Ergebnisse kurz zusammengefasst.

# 2 Mathematisches Modell zur Rekonstruktion

Wir stellen zunächst ein passendes mathematisches Modell auf, mit dem
wir die Daten $(Y_i, t_i)^n_{i=1}$ beschreiben:

$\mathbf{Y_i} = f(\mathbf{t_i}) + \mathbf{\epsilon_i}$

wobei f die zu rekonstruierende, glatte Funktion und
$\epsilon_1, … \epsilon_n$ Messfehler aufgrund des Messgeräts sind. Die
Messzeitpunkte $\mathbf{t} = t_1, ..., t_n$ sind die tatsächlichen
Erhebungszeitpunkte des Physikers, die wird als gegeben betrachten.

Für die Rekonstruktion treffen wir folgende Annahmen:

-   für beliebige Zeitpunkte $t^´_1, ..., t^´_m$ aus dem
    Erhebungsintervall [0,5] gilt
    $f(t^´) \sim \mathcal{}N(0, K_{t^´,t^´})$, wobei $K_{t^´, t^´}$ die
    Kovarianzmatrix - genauer Kernelmatrix[^1] - ist. Für alle
    $\mathbf{t} \in \mathbb{R^{n}}$ und $\mathbf{s} \in \mathbb{R^{m}}$
    werden Abstände mit der Gauß-Kernfunktion modelliert:

    ???

-   die Fehler $\mathbf{\epsilon_{1}}, ... \epsilon_{n}$ sind
    unabhängig, identisch $\mathcal{N}(0,\,\sigma^{2})$ verteilt und
    unabhängig von $f(\mathbf{t)}$.

    Die Modellvarianz $\gamma$ und die Fehlervarianz $\sigma^2$ können
    wir frei wählen, wobei immer gilt $\gamma > 0$ und $\sigma^2 > 0$.

[^1]: Die Kernelmatrix enthält die modellierten Kovarianzen zwischen
    jedem Paar von Messzeitpunkten. Zur Modellierung dieser Kovarianzen
    verwenden wir die Gaussche Kernfunktion $\exp(\frac{-s^2}{\gamma})$,
    die uns im Vergleich zu anderen Kernfunktionen eine möglichst glatte
    Funktion liefert. [[Gaussian processes (1/3) - From scratch
    (peterroelants.github.io)](https://peterroelants.github.io/posts/gaussian-process-tutorial/)]

Daraus folgt

\
$f(t^´| \mathbf{Y}=\mathbf{y}) \sim \mathcal{N(\mu_{2|1}, \Sigma_{2|1})}$

,wobei

$\mathbf\mu_{2|1} = \Sigma_{2,1}\Sigma_{1,1}^{-1}\mathbf{y}$

und
$\Sigma_{2|1} = \Sigma_{2,2}-\Sigma_{2,1}\Sigma_{1,1}^{-1}\Sigma_{1,2}$.

dass die Messungen $\mathbf{Y}$ aus den tatsächlich erhobenen Daten
ergänzt um $f(t^´)$ ebenfalls $\sim \mathcal{N(0,\Sigma)}$ , wobei ...

# 3 Kondition des mathematischen Problems

## 3.1 Konditionszahlen der Teilprobleme und allgemeine Schranke

Um besser zu verstehen, wie stark unsere Lösung $\mu_{2|1}$ von der
Störung der Eingangsdaten abhängt, betrachten wir zunächst die
Konditionierung der Teilprobleme.

Sei dazu $(\mu_{2|1}, \Sigma_{2,1}\Sigma_{1,1},\mathbf{y})$ ein Problem
mit Lösung $\mu_{2|1} = \Sigma_{2,1}\Sigma_{1,1}^{-1}{y}$, wobei
$\Sigma_{2,1} \in \mathbb{R^{{m\times}n}}$,
$\Sigma_{1,1} \in \mathbb{R^{n{\times}n}}$ und
$\mathbf{y} \in \mathbb{R^{n}}$ mit $\Sigma_{1,1}$ invertierbar. Sei
außerdem $|\cdot|$ eine zulässige Norm.

Wir definieren die folgenden drei Teilprobleme:

$\mu_{2|1}(\Sigma_{2,1}) = \Sigma_{2,1}\Sigma_{1,1}^{-1}{y}$ (1)

$\mu_{2|1}(\Sigma_{1,1}) = \Sigma_{2,1}\Sigma_{1,1}^{-1}{y}$ (2)

$\mu_{2|1}(\mathbf{y}) = \Sigma_{2,1}\Sigma_{1,1}^{-1}{y}$ (3).

Dann gilt:

$\frac{|{\tilde\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y} - {\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} = \frac{|({\tilde\Sigma_{2,1}} - {\Sigma_{2,1}})\Sigma_{1,1}^{-1}\mathbf{y}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} \leq^{*1} \frac{|{\tilde\Sigma_{2,1}} - {\Sigma_{2,1}}||\Sigma_{1,1}^{-1}\mathbf{y}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} = \frac{|{\Sigma_{2,1}}||\Sigma_{1,1}^{-1}\mathbf{y}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} \frac{|{\tilde\Sigma_{2,1}} - {\Sigma_{2,1}}|}{|{\Sigma_{2,1}}|} = \kappa_1 \frac{|{\tilde\Sigma_{2,1}} - {\Sigma_{2,1}}|}{|{\Sigma_{2,1}}|}$
mit $\kappa_1$ als Konditionszahl für (1)

$\frac{|{\Sigma_{2,1}}\tilde\Sigma_{1,1}^{-1}\mathbf{y} - \Sigma_{2,1}\Sigma_{1,1}^{-1}\mathbf{y}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} \leq \frac{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}(\tilde{\mathbf{y}} - \mathbf{y})|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} \leq^{*1} \frac{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}||\tilde{\mathbf{y}} - \mathbf{y}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} = \frac{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}||\mathbf{y}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} \frac{|{\tilde{y}} - {y}|}{|{y}|} = \kappa_3 \frac{|{\tilde{y}} - {y}|}{|{y}|}$
mit $\kappa_3$ als Konditionszahl für (3)

$\frac{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y} - {\Sigma_{2,1}}\Sigma_{1,1}^{-1}\tilde{\mathbf{y}}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} \leq^{*1} \frac{|{\Sigma_{2,1}}||\tilde\Sigma_{1,1}^{-1}-\Sigma_{1,1}^{-1}||\mathbf{y}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} \leq^{*2} \frac{|\Sigma_{1,1}^{-1}|^{2}|{\Sigma_{2,1}}||\mathbf{y}||\Sigma_{1,1}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} \frac{|\tilde\Sigma_{1,1}^{-1} - \Sigma_{1,1}^{-1}|}{|\Sigma_{1,1}|} \leq |\Sigma_{1,1}||\Sigma_{1,1}^{-1}| \frac{|\Sigma_{1,1}^{-1}||{\Sigma_{2,1}}||\mathbf{y}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} \frac{\tilde\Sigma_{1,1}^{-1} - \Sigma_{1,1}^{-1}}{\Sigma_{1,1}^{-1}} = \kappa_2\frac{\tilde\Sigma_{1,1}^{-1} - \Sigma_{1,1}^{-1}}{\Sigma_{1,1}^{-1}}$
mit $\kappa_2$ Konditionierungszahl für (2) als Produkt der
Konditionszahl $|\Sigma_{1,1}||\Sigma_{1,1}^{-1}|$ für die Berechnung
der Inversen von $\Sigma_{1,1}$ und der Konditionszahl
$\frac{|\Sigma_{1,1}^{-1}||{\Sigma_{2,1}}||\mathbf{y}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|}$
für die Matrixmultiplikation.

Die allgemeine Schranke für den (relativen) Outputfehler bezüglich aller
relativen Inputfehler ergibt sich aus der Summe der einzelnen Produkte
von Konditionszahl $\kappa_{\{1,2,3\}}$ und jeweiligem Fehler.
(BEGRÜNDUNG)

$*^1$ Submultiplikativität

$*^2$ Folgt aus Aufgabe 2.1c) der Übung inkl. Beweis zur Stetigkeit der
Matrixinvertierung in der Lösung

Theorem 1.1

Sei $A \in \mathbb{R^{n{\times}n}}$ invertierbar. Sei außerdem $|\cdot|$
eine zulässige Norm. Dann gilt für die (relative) Kondition:

$\kappa(A) = ||A||||A^{-1}||$, wobei die Kondition von der gewählten
Matrixnorm abhängt.

## 3.2 Einfluss der Hyperparameter auf die Konditionierung

Da die Konditionszahl des Gesamtproblems von der Kondition der
Kernmatrizen für $\Sigma_{1,1}$ und $\Sigma_{2,1}$ abhängt und diese
wiederum von der Kernfunktion $\exp(\frac{-s^2}{\gamma})$ abhängig sind,
betrachten wir zunächst den Einfluss von $\gamma$ auf die Konditionszahl
der Kernfunktion. Diese ergibt sich aus der Ableitung der Kernfunktion
nach $\gamma$:

$\kappa_{abs} = |\frac{\delta exp(\frac{-s^2}{\gamma})}{\delta\gamma}| = |exp(\frac{-s^2}{\gamma})\frac{s^2}{\gamma^2}| = exp(\frac{-s^2}{\gamma})\frac{s^2}{\gamma^2}$

$\rightarrow \kappa_{rel} = \frac{|\gamma|}{|exp(\frac{-s^2}{\gamma})|}exp(\frac{-s^2}{\gamma})\frac{s^2}{\gamma^2} = \frac{\gamma s^2}{\gamma^2} = \frac{s^2}{\gamma}$

Folglich ist für Werte von $\gamma \approx 0$ die Konditionszahl der
Kernfunktion groß und das Problem damit schlecht konditioniert. Für
$\gamma \geq s^2$ ist die Konditionszahl $\leq 1$ und das Problem
folglich gut konditioniert.

Da der Hyperparameter $\sigma$ nur zur Berechnung von $\Sigma_{1,1}$
verwendet wird, betrachten wir uns die Konditionszahl dieser Matrix,
wobei $\sigma_{max}$ maximaler Singulärwert und $\sigma_{min}$ minimaler
Singulärwert ist:

$\kappa_{\Sigma_{1,1}} = \frac{\sigma_{max}}{\sigma_{min}} = \frac{\Sigma_{1,1}}{\sigma_{min}} =^{*} \frac{K_{t,t} + \sigma^2I}{\sigma^2} > 1$

Folglich ist das Problem für $\sigma^2 \approx 0$ schlecht und für
$K_{t,t} \approx 0$ gut konditioniert.

$^* \sigma_{min} = \sigma^2$ folgt aus Beobachtung.

# 4 Algorithmus zur Lösung des Problems

Um den Algorithmus zu schreiben, der $\mu_{2|1}$ berechnet und unser
Problem löst, erstellen wir zunächst die Kernelfunktion sowie die
Funktion für unsere Kernelmatrix.

```{r}
# Kernfun: 
# Kernfun ist die Kernelfunktion, die als Prior dient und die Kovarianz für jede einzelne Kombination von Messzeitpunkten modelliert 
# Input: 
# d: absolute Differenz s zwischen zwei Messzeitpunkten
# gamma: beliebig wählbar, > 0
# Output: modellierte Kovarianz
Kernfun <- function(d, gamma) {
  exp(-(d^2) / gamma)
}

# matK: 
# matK ist die Kernelmatrix
# Input: 
## t: Vektor der Länge n, der Messzeitpunkte enthält
## s: Vektor mit Länge m, der Messzeitpunkte enthält
## gamma: beliebig wählbar, Input für die Kernelfunktion
# Output: eine Matrix n x m mit den modellierten Kovarianzen aus der Kernelfunktion als Einträge
matK <-
  function(t, s, gamma) {
    Kernfun(abs(
      matrix(t, nrow = length(t), ncol = length(s)) - matrix(
        s,
        nrow = length(t),
        ncol = length(s),
        byrow = TRUE
      )
    ), gamma)
  }
```

## 4.1 Unter Verwendung der Inversen

```{r}
alg_Inv <- function(t.Strich,
                 gamma = 1,
                 sigma.quadrat = 1) {
  Sigma1.1 <-
    matK(messungen$t, messungen$t, gamma) + sigma.quadrat * diag(length(messungen$t))
  Sigma2.1 <- t(matK(messungen$t, t.Strich, gamma))
  y <- Sigma2.1 %*% solve(Sigma1.1) %*% messungen$y
  y
}
```

Dieser Algorithmus ist schlecht, weil er die Inverse einer Matrix
verwendet. Ein Algorithmus, der eine Inverse verwendet, ist im
Allgemeinen instabiler als ein Algorithmus ohne Inverse, da die Inverse
nicht immer eindeutig ist. Auch wenn es nur eine Inverse gibt, kann der
Algorithmus immernoch anfälliger sein, weil die Berechnung einer
Inversen in der Regel numerisch anspruchsvoller ist als die Berechnung
anderer Funktionen. Die Verwendung einer Inversen kann daher dazu
führen, dass kleine numerische Fehler, die während der Berechnungen
auftreten, sich verstärken und zu größeren Fehlern in den endgültigen
Ergebnissen führen. Insgesamt ist es daher im Allgemeinen ratsam,
Algorithmen zu vermeiden, die eine Inverse verwenden, wenn es möglich
ist. Deshalb schreiben wir im Folgenden einen alternativen Algorithmus,
der die Berechnung der Inversen mithilfe der Cholesky Zerlegung
vermeidet.

## 4.2 Unter Verwendung der Cholesky-Zerlegung

Bei $\Sigma_{1,1}$ handelt es sich um eine Kovarianzmatrix. Folglich ist
$\Sigma_{1,1}$ symmetrisch und positiv definit und damit eine
SPD-Matrix. Das erlaubt es uns die Cholesky-Zerlegung zu verwenden und
$\Sigma_{1,1}$ zu $\Sigma_{1,1} = LL^t$ zu zerlegen, wobei $L$ eine
untere Dreiecksmatrix ist. Nun können wir durch Vorwärts- und
Rückwärtssubstitution das LGS lösen. Für dieses Vorgehen verwenden wir
in R die drei Subroutinen chol, forwardsolve und backsolve, mithilfe
derer wir das Invertieren der Matrix $\Sigma_{1,1}$ vermeiden und so
erheblichen Rechen- und Speicheraufwand einsparen.

```{r}
# alg_CH
# berechnet mü2.1 mittels Cholesky Zerlegung
# Input: 
## t.Strich: Vektor der Länge m mit Messzeitpunkten
## sigma.quadrat > 0 mit default = 1
## gamma > 0 mit default = 1
# Output:
## Vektor mü2.1 der Länge m

alg_CH <- function(t.Strich, sigma.quadrat = 1, gamma = 1){
  Sigma1.1 <-
    matK(messungen$t, messungen$t, gamma) + sigma.quadrat * diag(length(messungen$t))
  Sigma2.1 <- t(matK(messungen$t, t.Strich, gamma))
  L <- chol(Sigma1.1)
  B <- forwardsolve(t(L), messungen$y)
  Sigma2.1 %*% backsolve(L, B)
}
```

## 4.3 Laufzeit-Komplexität des Algorithmus unter Verwendung der Cholesky-Zerlegung

Zur Berechnung der Laufzeit-Komplexität berücksichtigen wir Addition,
Subtraktion, Multiplikation und Division als elementare
Rechenoperationen, wobei der Mehraufwand der Division gegenüber der
Multiplikation vernachlässigt wird.

Die Laufzeit-Komplexität des Algorithmus aus 4.2 ergibt sich aus den
folgenden Teil-Laufzeit-Komplexitäten, wobei m und n durch die Größe der
Input-Daten bestimmt werden:

-   Erstellen der mxn Matrix $\Sigma_{2,1}$: $\mathcal{O}(mn)$ (4
    Rechenoperationen auf jedem der mn Einträge)

-   Erstellen der nxn Matrix $\Sigma_{1,1}$: $\mathcal{O}(n^2)$ (4
    Rechenoperationen auf jedem der $n^2$ Einträge)

-   Cholesky-Zerlegung von $\Sigma_{1,1}$: $\mathcal{O}(n^3)$ ([Parker,
    M., Digital Signal Processing 101 (Second
    Edition)](https://www.sciencedirect.com/topics/engineering/cholesky-decomposition))

-   Vorwärts- und Rückwärtssubstitution: $\mathcal{O}(n^2)$ (gemäß
    Vorlesung)

-   Multiplikation einer mxn Matrix mit einem n-dimensionalen Vektor:
    $\mathcal{O}(mn)$ (pro Zeile n Multiplikationen und Aufsummieren,
    für m Zeilen)

Die Laufzeit-Komplexität beträgt also insgesamt
$\mathcal{O}(mn) + \mathcal{O}(n^2) + \mathcal{O}(n^3) + \mathcal{0}(n^2) + \mathcal{O}(mn) = \mathcal{O}(n^3 + mn)$.
Daraus folgt, dass sich die Laufzeit ungefähr auf das $2^3$-fache
erhöht, wenn sich n verdoppelt. Wenn sich m verdoppelt, verdoppelt das
die Laufzeit. In anderen Worten: die Laufzeit wächst polynomiell vom
Grad 3 mit n und linear mit m.

```{r}
grid <- expand.grid(
  m = seq.int(1000, 100000, length = 3),
  n = seq.int(1000, 100000, length = 3)
)

times1 <- numeric(nrow(grid))
times2 <- numeric(nrow(grid))

for (k in 1:nrow(grid)) {
m <- grid[k, 1]
n <- grid[k, 2]
t.Strich <- seq(from = 0, to = 5, l = length(m))
times1[k] <- median(microbenchmark(alg_CH(t.Strich, sigma.quadrat = 1, gamma = 1), times = 10)$time)
times2[k] <- median(microbenchmark(alg_Inv(m, sigma.quadrat = 1, gamma = 1), times = 10)$time)
}
```

```{r}
cbind(grid, time = times1) |>
ggplot(aes(m, time, color = as.factor(n))) +
geom_line()
```

```{r}
cbind(grid, time = times2) |>
ggplot(aes(m, time, color = as.factor(n))) +
geom_line()
```

# 5 Implementierung des Algorithmus

Die verrauschten Messdaten des Physikers sind im Datensatz `messungen`
mit den Spalten t und y enthalten und sehen folgendermaßen aus:

```{r}
plot(messungen$t, messungen$y, xlab = "Messzeitpunkte t", ylab = "Messungen y", main = "Verrauschte Daten")
```

## 5.1 Rekonstruktion der unverrauschten Daten

Zur Rekonstruktion verwenden wir den stabilen und effizienten
Algorithmus aus 4.2, der sich der Cholesky-Zerlegung bedient. Für die
Rekonstruktion wählen wir 100 Messzeitpunkte aus dem Zeitintervall
[0,5], aus dem uns die Messungen des Physikers vorliegen.

```{r}
t.Strich <- seq(from = 0, to = 5, length.out = 100)
```

```{r}
curve(alg_CH(x, sigma.quadrat = 1, gamma = 1), from = 0, to = 5, col = "red", ylim = c(0, max(messungen$y, alg_CH(t.Strich))), ylab = "y", xlab = "t.Strich")
points(messungen$t, messungen$y)
```

## 5.2 Einfluss der Hyperparameter $\sigma$ und $\gamma$ auf die Rekonstruktion

### 5.2.1 Einfluss auf den Verlauf von $\hat{f}$ 

In 3.2 wurde bereits der Einfluss der Hyperparameter auf die
Konditionierung des Problems untersucht. Im Folgenden soll der Einfluss
der Hyperparameter auf die Rekonstruktionsfunktion näher analysiert
werden. Dazu betrachten wir zunächst einige Beispiele Verlauf der
geglätteten Kurve für verschiedene Werte von $\sigma$ und $\gamma$. Rein
visuell erhalten wir mit den Werten $\sigma = \gamma = 1$ den besten Fit
der Kurve, wie wir bereits in 5.1 gesehen haben (immer in Rot
geplottet). Wählen wir nun für festes $\sigma = 1$ unser $\gamma < 1$,
so geht das auf Kosten der glatten Kurve und die rekostruierte Funktion
$\hat{f}$ springt mit sinkendem gamma immer stärker, wie man in
Abbildung XX erkennen kann. Bei einem sehr kleinen Wert von
$\gamma = 0.000001$ hebt sich die Funktion kaum noch vom Prior = 0 ab,
den wir in Kapitel 2 angenommen hatten. Wählen wir für festes
$\sigma = 1$ hingegen Werte \> 1, so flacht die rekonstruierte Funktion
$\hat{f}$ mit wachsendem gamma immer stärker ab, wie in Abbildung XX zu
sehen. Bei einem sehr großen Wert von $\gamma = 1000$ ist $\hat{f}$
nahezu konstant mit $\hat{f} \approx \bar{x} = 20.70342$.

```{r}
curve(alg_CH(x, sigma.quadrat = 1, gamma = 1), from = 0, to = 5, col = "red", ylim = c(0, max(messungen$y, alg_CH(t.Strich))), ylab = "y", xlab = "t.Strich")
curve(alg_CH(x, sigma.quadrat = 1, gamma = 0.1), from = 0, to = 5, col = "darkolivegreen4", ylim = c(0, max(messungen$y, alg_CH(t.Strich))), ylab = "y", xlab = "t.Strich", add = TRUE)
curve(alg_CH(x, sigma.quadrat = 1, gamma = 0.01), from = 0, to = 5, col = "darkolivegreen3", ylim = c(0, max(messungen$y, alg_CH(t.Strich))), ylab = "y", xlab = "t.Strich", add = TRUE)
curve(alg_CH(x, sigma.quadrat = 1, gamma = 0.000001), from = 0, to = 5, col = "darkolivegreen2", ylim = c(0, max(messungen$y, alg_CH(t.Strich))), ylab = "y", xlab = "t.Strich", add = TRUE)
points(messungen$t, messungen$y)
```

```{r}
curve(alg_CH(x, sigma.quadrat = 1, gamma = 1), from = 0, to = 5, col = "red", ylim = c(0, max(messungen$y, alg_CH(t.Strich))), ylab = "y", xlab = "t.Strich")
curve(alg_CH(x, sigma.quadrat = 1, gamma = 10), from = 0, to = 5, col = "darkgoldenrod4", ylim = c(0, max(messungen$y, alg_CH(t.Strich))), ylab = "y", xlab = "t.Strich", add = TRUE)
curve(alg_CH(x, sigma.quadrat = 1, gamma = 100), from = 0, to = 5, col = "darkgoldenrod", ylim = c(0, max(messungen$y, alg_CH(t.Strich))), ylab = "y", xlab = "t.Strich", add = TRUE)
curve(alg_CH(x, sigma.quadrat = 1, gamma = 1000), from = 0, to = 5, col = "darkgoldenrod1", ylim = c(0, max(messungen$y, alg_CH(t.Strich))), ylab = "y", xlab = "t.Strich", add = TRUE)
points(messungen$t, messungen$y)
```

Wählen wir nun für festes $\gamma = 1$ unser $\sigma < 1$, so erhöht
sich die Varianz der rekonstruierten Kurve $\hat{f}$ mit sinkenden
Werten von $\sigma$, wie man in Abbildung XX erkennen kann. Wählen wir
dagegen für festes $\gamma = 1$ unser $\sigma > 1$, so sinkt die Varianz
der rekontruierten Kurve, wie in Abbildung XX zu sehen.

```{r}
curve(alg_CH(x, sigma.quadrat = 1, gamma = 1), from = 0, to = 5, col = "red", ylim = c(0, max(messungen$y, alg_CH(t.Strich))), ylab = "y", xlab = "t.Strich")
curve(alg_CH(x, sigma.quadrat = 0.1, gamma = 1), from = 0, to = 5, col = "darkolivegreen4", ylim = c(0, max(messungen$y, alg_CH(t.Strich))), ylab = "y", xlab = "t.Strich", add = TRUE)
curve(alg_CH(x, sigma.quadrat = 0.01, gamma = 1), from = 0, to = 5, col = "darkolivegreen3", ylim = c(0, max(messungen$y, alg_CH(t.Strich))), ylab = "y", xlab = "t.Strich", add = TRUE)
curve(alg_CH(x, sigma.quadrat = 0.00000000001, gamma = 1), from = 0, to = 5, col = "darkolivegreen2", ylim = c(0, max(messungen$y, alg_CH(t.Strich))), ylab = "y", xlab = "t.Strich", add = TRUE)
points(messungen$t, messungen$y)
```

```{r}
curve(alg_CH(x, sigma.quadrat = 1, gamma = 1), from = 0, to = 5, col = "red", ylim = c(0, max(messungen$y, alg_CH(t.Strich))), ylab = "y", xlab = "t.Strich")
curve(alg_CH(x, sigma.quadrat = 10, gamma = 1), from = 0, to = 5, col = "darkgoldenrod4", ylim = c(0, max(messungen$y, alg_CH(t.Strich))), ylab = "y", xlab = "t.Strich", add = TRUE)
curve(alg_CH(x, sigma.quadrat = 100, gamma = 1), from = 0, to = 5, col = "darkgoldenrod", ylim = c(0, max(messungen$y, alg_CH(t.Strich))), ylab = "y", xlab = "t.Strich", add = TRUE)
curve(alg_CH(x, sigma.quadrat = 1000, gamma = 1), from = 0, to = 5, col = "darkgoldenrod1", ylim = c(0, max(messungen$y, alg_CH(t.Strich))), ylab = "y", xlab = "t.Strich", add = TRUE)
points(messungen$t, messungen$y)
```

### 5.2.1 Versagen des Algorithmus durch bestimmte Hyperparameter

Nachdem wir den generellen Einfluss verschiedener Hyperparamter auf den
Verlauf der Rekonstruktion besser verstanden haben , betrachten wir nun
Werte für $\sigma$ und $\gamma$, die dazu führen, dass unser Algorithmus
versagt. Zum einen führen bestimmte Werte der Hyperparameter dazu, dass
der Algorithmus nicht mehr ausführbar ist. Wählt man beispielsweise
$\sigma = 1e-08$ beziehungsweise $\sigma^2 = 1e-16$, so erhält man
folgende Fehlermeldung:

```{r}
curve(alg_CH(x, sigma.quadrat = 1e-16, gamma = 1), from = 0, to = 5, col = "red", ylim = c(0, max(messungen$y, alg_CH(t.Strich))), ylab = "y", xlab = "t.Strich")
points(messungen$t, messungen$y)
```

Weil $\sigma^2 = 1e-16$ unter der ... von ... liegt, sind die Eigenwerte
der Matrix $\Sigma_{1,1} \approx 0$, sodass die Matrix nicht mehr
positiv definit ist und damit die Voraussetzung der SPD-Matrix für die
Cholesky-Zerlegung verletzt wird.

Für ... funktioniert Chol \_ alg noch. Für Inv allerdings nicht mehr

```{r}
curve(alg_CH(x, sigma.quadrat = 1e-15, gamma = 1), from = 0, to = 5, col = "red", ylim = c(0, max(messungen$y, alg_CH(t.Strich))), ylab = "y", xlab = "t.Strich")
curve(alg_Inv(x, sigma.quadrat = 1e-15, gamma = 1), from = 0, to = 5, col = "blue", ylim = c(0, max(messungen$y, alg_CH(t.Strich))), ylab = "y", xlab = "t.Strich", add = TRUE)
points(messungen$t, messungen$y)
```

Zum anderen gibt es besondere Zahlen, die uns ein Ergebnis ohne Mehrwert
liefern.

???

Nicht zuletzt kann man das Versagen eines Algorithmus auch so
interpretieren, dass die Laufzeit des Algorithmus inakzeptabel ist.
Vergleicht man an dieser Stelle den Algorithmus, der die
Cholesky-Zerlegung verwendet mit dem aus 4.1, der die Inverse verwendet,
so fällt schnell auf, dass die Laufzeit für zweiteren Algorithmus mit
der Größe der Daten stark ansteigt und immer über dem Algorithmus mit
Cholesky-Zerlegung liegt.

```{r}
microbenchmark(alg_CH(x, sigma.quadrat = 10^100, gamma = 10^100))
```

```{r}
# Define the function to compute MSE
mse <- function(gamma) {
  # Compute MSE for a given sigma.quadrat value
  mean(messungen$y - alg_CH(messungen$t, sigma.quadrat = 1, gamma))^2 + var(messungen$y - alg_CH(messungen$t, sigma.quadrat = 1, gamma))^2
}

# Generate a sequence of sigma.quadrat values
gammas <- seq(from = 0.0001, to = 100, by = 100)

# Compute the MSE for each sigma.quadrat value
mse_values <- mapply(mse, gammas)

# Plot the MSE values using the curve function
plot(mse_values, xlim=c(-3, 100), ylim=c(-1, 120), col="red", xlab="Gamma", ylab="MSE", type = "l")
```

# Fazit

messungen\$t als Argument in den Algorithmus \> Laufzeit

Vergleich Laufzeit von alg_inv

Maschinengenauigkeit?

Gliederung: 1) Problem 2) Alg-Alternativen, Implementierung, Evaluation?

@Eugen Intuition sigma\^2

Abbildungen in den Text einbeziehen

Interpretation Laufzeit Komplexitäten

Laufzeiten Plots

Ist m wirklich wichtig für die Konditionierung?

Kondition, Stabilität, Effizienzbetrachtung

Betragsstriche

sigma.quadrat.quadrat / sigma.quadrat

Im Folgenden werden wir uns deshalb Zerlegungen von sigma.quadrat1.1
zunutze machen, um y ohne Inverse zu berechnen.

code verstecken

Um den Einfluss der Hyperparameter $\gamma$ und $\sigma.quadrat$ näher
zu untersuchen, visualisieren wir zunächst deren Einfluss auf die
Konditionszahl von $\sigma.quadrat_{1,1}$ für festes $\sigma = 1$ bzw.
festes $\gamma = 1$.

Achsenbeschriftung

```{r}
# sigma.quadrat <- 1
# 
# kappa.fun1 <- function(gamma) {
#   Kernfun <- function(s) {
#     exp(-(s ^ 2) / gamma)
#   }
#   matK <-
#     function(t, s) {
#       Kernfun(abs(
#         matrix(t, nrow = length(t), ncol = length(s)) - matrix(
#           s,
#           nrow = length(t),
#           ncol = length(s),
#           byrow = TRUE
#         )
#       ))
#     }
#   Sigma1.1 <-
#     matK(messungen$t, messungen$t) + sigma.quadrat * diag(length(messungen$t))
#   return(max(svd(Sigma1.1)$d) / min(svd(Sigma1.1)$d))
# }
# 
# plot(gamma, sapply(gamma, kappa.fun1), type = "l", xlab = "gamma", ylab = "Konditionszahl")
# 
# 
# 
# gamma <- seq(0.1, 10, by = .1)

```

`{# {r} # gamma <- 1 #  # kappa.fun2 <- function(sigma.quadrat) { #   Kernfun <- function(s) { #     exp(-(s ^ 2) / gamma) #   } #   matK <- #     function(t, s) { #       Kernfun(abs( #         matrix(t, nrow = length(t), ncol = length(s)) - matrix( #           s, #           nrow = length(t), #           ncol = length(s), #           byrow = TRUE #         ) #       )) #     } #   Sigma1.1 <- #     matK(messungen$t, messungen$t) + sigma.quadrat * diag(length(messungen$t)) #   return(max(svd(Sigma1.1)$d) / min(svd(Sigma1.1)$d)) # } #  # plot(sigma.quadrat, sapply(sigma.quadrat, kappa.fun2), type = "l", xlab = "sigma", ylab = "Konditionszahl") #  #  # sigma.quadrat <- seq(0.1, 10, by = .1)`

Wie verändert sich die Laufzeitkomplexität für die Längen 1, 10, 100,
1000 von t.Strich?

```{r}
t.Strich.1 <- seq(from = 0, to = 5, l = 1)

t.Strich.10 <- seq(from = 0, to = 5, l = 10)

t.Strich.100 <- seq(from = 0, to = 5, l = 100)

t.Strich.1000 <- seq(from = 0, to = 5, l = 1000)
```

Der Algorithmus, der die Cholesky-Zerlegung verwendet, kann in die
folgenden größeren Operationen aufgeteilt werden:

```{r}
grid.HP <- expand.grid(
  sigma.quadrat = seq.int(1, 100, length = 4),
  gamma = seq.int(1, 100, length = 4)
)

for (k in 1:nrow(grid.HP)) {
  sigma.quadrat <- grid.HP[k, 1]
  gamma <- grid.HP[k, 2]
  y <- alg_CH(t.Strich, sigma.quadrat, gamma)}



# Add a column with the function values
grid.HP$y <- alg_CH(t.Strich, sigma.quadrat = grid.HP$sigma.quadrat, gamma = grid.HP$gamma)

# Plot the function using color to encode the different sigma.quadrat values
plot(y ~ sigma.quadrat, data = df, col = as.factor(df$sigma.quadrat))

```

## 4.3 Unter Verwendung der Orthogonalen Diagonalisierung

Die Matrix Sigma1.1 ist symmetrisch. Folglich können wir sie orthogonal
diagonalisieren und damit die Berechnung der Inversen vermeiden, da für
orthogonale Matrizen gilt: solve(A) = t(A) und die Inverse einer
Diagonalmatrix sich einfach ergibt, indem man die Kehrbrüche auf der
Diagonalen berechnet.

```{r}
# alg_OD
# Berechnet mü2.1 mithilfe orthogonaler Diagonalisierung
# Input: 
## t.Strich: Vektor der Länge m mit Messzeitpunkten
## sigma.quadrat > 0 mit default = 1
## gamma > 0 mit default = 1
# Output:
## Vektor mü2.1 der Länge m

alg_OD <- function(t.Strich, sigma.quadrat = 1, gamma = 1){
  Sigma1.1 <-
    matK(messungen$t, messungen$t, gamma) + sigma.quadrat * diag(length(messungen$t))
  Sigma2.1 <- t(matK(messungen$t, t.Strich, gamma))
  P <- GramSchmidt(eigen(Sigma1.1)$vectors)  #Orthogonalisierung und Normalisierung mithilfe Gram Schmidt Algorithmus aus matlib package
  D.inv <- diag(1/eigen(Sigma1.1)$values)  
  y <- Sigma2.1 %*% t(P) %*% D.inv %*% P %*% messungen$y
  y
}
```

-   Pro Zeile der Matrix $\Sigma_{2,1}$: n Multiplikationen und (n-1)
    Additionen für m Zeilen, demnach:
    $m(n + (n-1)) = 2mn -m = mn - m = mn$

<!-- -->

-   Pro Eintrag der Matrix werden eine Subtraktion, zwei
    Multiplikationen und eine Division durchgeführt und das für jeden
    der mn Einträge

    Demnach: 4mn

    Subtraktion jedes Eintrags des Vektors $\mathbf{t^`}$ mit Länge m
    von jedem Eintrag des Vektors $\mathbf{s}$ mit Länge n und
    anschließende Rechenoperationen (Multiplikation, Division) auf jedem
    der m\*n Einträge, die aber keine höhere Laufzeit-Komplexität als
    $\mathcal{O}(mn)$ haben.

-   Subtraktion jedes Eintrags des Vektors $\mathbf{t}$ mit Länge n von
    jedem Eintrag desselben Vektors und anschließende Rechenoperationen
    (Multiplikation, Division) auf jedem der $n^2$ Einträge, die aber
    keine höhere Laufzeit-Komplexität als $\mathcal{O}(n^2)$ haben.

-   

```{r}
grid.HP <- expand.grid(
  m = seq.int(1, 100, length = 4),
  n = seq.int(1, 100, length = 4)
)

plot <- curve(alg_CH(x, sigma.quadrat, gamma), from = 0, to = 5, col = "red", ylim = c(0, max(messungen$y, alg_CH(t.Strich))), ylab = "y", xlab = "t.Strich")


sigma.kat <- numeric(nrow(grid))
gamma.kat <- numeric(nrow(grid))

for (k in 1:nrow(grid)) {
sigma.quadrat <- grid[k, 1]
gamma <- grid[k, 2]
plot <- curve(alg_CH(x, sigma.quadrat, gamma), from = 0, to = 5, col = "grey", ylim = c(0, max(messungen$y, alg_CH(t.Strich))), ylab = "y", xlab = "t.Strich", add = TRUE)}

cbind(grid, HP = sigma.kat) |>
ggplot(aes(t.Strich, y, color = as.factor(m))) +
geom_line()
```
