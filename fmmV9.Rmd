---
title: ""
author: "Cosima Froehner"
date: "2022-12-16"
output:
  pdf_document: default
  toc: true
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
getwd()
messungen <- read.csv("messungen.csv")
library("matlib")
library("pracma")
library("microbenchmark")
library("ggplot2")
```

# 1 Noisy Data

Ein Physiker hat ein Experiment durchgeführt, um die Auswirkungen eines
elektromagnetischen Impulses zu erforschen. Er hatte dabei eine schöne,
glatte Kurve erwartet. Allerdings stellt sich das Messgerät als ungenau
heraus und die tatsächlichen Messungen sind stark verrauscht. Der
vorliegende Report beschäftigt sich nun mit dem Korrigieren des
Messfehlers und der Rekonstruktion einer glatten Kurve als Näherung für
die unverrauschten Messdaten. Dazu soll im Folgenden zunächst die
Kondition des Problems analysiert werden. Im Anschluss wird zur Lösung
des Problems ein Algorithmus geschrieben und implementiert, dessen
Laufzeit-Komplexität hergeleitet und mit einem weniger stabilen
Algorithmus für das Problem verglichen wird. Abschließend werden die
Ergebnisse kurz zusammengefasst.

# 2 Mathematisches Modell zur Rekonstruktion

Wir stellen zunächst ein passendes mathematisches Modell auf, mit dem
wir die Daten $(Y_i, t_i)^n_{i=1}$ beschreiben:

$\mathbf{Y_i} = f(\mathbf{t_i}) + \mathbf{\epsilon_i}$

wobei f die zu rekonstruierende, glatte Funktion und
$\epsilon_1, … \epsilon_n$ Messfehler aufgrund des Messgeräts sind. Die
Messzeitpunkte $\mathbf{t} = t_1, ..., t_n$ sind die tatsächlichen
Erhebungszeitpunkte des Physikers, die wird als gegeben betrachten.

Für die Rekonstruktion treffen wir folgende Annahmen:

-   für beliebige Zeitpunkte $t^´_1, ..., t^´_m$ aus dem
    Erhebungsintervall [0,5] gilt
    $f(t^´) \sim \mathcal{}N(0, K_{t^´,t^´})$, wobei $K_{t^´, t^´}$ die
    Kovarianzmatrix - genauer Kernelmatrix[^1] - ist. Für alle
    $\mathbf{t} \in \mathbb{R^{n}}$ und $\mathbf{s} \in \mathbb{R^{m}}$
    werden Abstände mit der Gauß-Kernfunktion modelliert:

    ???

-   die Fehler $\mathbf{\epsilon_{1}}, ... \epsilon_{n}$ sind
    unabhängig, identisch $\mathcal{N}(0,\,\sigma^{2})$ verteilt und
    unabhängig von $f(\mathbf{t)}$.

    Die Modellvarianz $\gamma$ und die Fehlervarianz $\sigma^2$ können
    wir frei wählen.

[^1]: Die Kernelmatrix enthält die modellierten Kovarianzen zwischen
    jedem Paar von Messzeitpunkten. Zur Modellierung dieser Kovarianzen
    verwenden wir die Gaussche Kernfunktion $\exp(\frac{-s^2}{\gamma})$,
    die uns im Vergleich zu anderen Kernfunktionen eine möglichst glatte
    Funktion liefert. [[Gaussian processes (1/3) - From scratch
    (peterroelants.github.io)](https://peterroelants.github.io/posts/gaussian-process-tutorial/)]

Daraus folgt

\
$f(t^´| \mathbf{Y}=\mathbf{y}) \sim \mathcal{N(\mu_{2|1}, \Sigma_{2|1})}$

,wobei

$\mathbf\mu_{2|1} = \Sigma_{2,1}\Sigma_{1,1}^{-1}\mathbf{y}$

und
$\Sigma_{2|1} = \Sigma_{2,2}-\Sigma_{2,1}\Sigma_{1,1}^{-1}\Sigma_{1,2}$.

dass die Messungen $\mathbf{Y}$ aus den tatsächlich erhobenen Daten
ergänzt um $f(t^´)$ ebenfalls $\sim \mathcal{N(0,\Sigma)}$ , wobei ...

# 3 Konditionierung des mathematischen Problems

## 3.1 Konditionszahlen der Teilprobleme und allgemeine Schranke

Um besser zu verstehen, wie stark unsere Lösung $\mu_{2|1}$ von der
Störung der Eingangsdaten abhängt, betrachten wir zunächst die
Konditionierung der Teilprobleme.

Sei dazu $(\mu_{2|1}, \Sigma_{2,1}\Sigma_{1,1},\mathbf{y})$ ein Problem
mit Lösung $\mu_{2|1} = \Sigma_{2,1}\Sigma_{1,1}^{-1}{y}$, wobei
$\Sigma_{2,1} \in \mathbb{R^{{n\times}n}}$,
$\Sigma_{1,1} \in \mathbb{R^{n{\times}n}}$ und
$\mathbf{y} \in \mathbb{R^{n}}$ mit $\Sigma_{1,1}$ invertierbar. Sei
außerdem $|\cdot|$ eine zulässige Norm.

Wir definieren die folgenden drei Teilprobleme:

$\mu_{2|1}(\Sigma_{2,1}) = \Sigma_{2,1}\Sigma_{1,1}^{-1}{y}$ (1)

$\mu_{2|1}(\Sigma_{1,1}) = \Sigma_{2,1}\Sigma_{1,1}^{-1}{y}$ (2)

$\mu_{2|1}(\mathbf{y}) = \Sigma_{2,1}\Sigma_{1,1}^{-1}{y}$ (3).

Dann gilt:

$\frac{|{\tilde\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y} - {\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} = \frac{|({\tilde\Sigma_{2,1}} - {\Sigma_{2,1}})\Sigma_{1,1}^{-1}\mathbf{y}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} \leq^{*1} \frac{|{\tilde\Sigma_{2,1}} - {\Sigma_{2,1}}||\Sigma_{1,1}^{-1}\mathbf{y}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} = \frac{|{\Sigma_{2,1}}||\Sigma_{1,1}^{-1}\mathbf{y}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} \frac{|{\tilde\Sigma_{2,1}} - {\Sigma_{2,1}}|}{|{\Sigma_{2,1}}|} = \kappa_1 \frac{|{\tilde\Sigma_{2,1}} - {\Sigma_{2,1}}|}{|{\Sigma_{2,1}}|}$
mit $\kappa_1$ als Konditionszahl für (1)

$\frac{|{\Sigma_{2,1}}\tilde\Sigma_{1,1}^{-1}\mathbf{y} - \Sigma_{2,1}\Sigma_{1,1}^{-1}\mathbf{y}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} \leq \frac{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}(\tilde{\mathbf{y}} - \mathbf{y})|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} \leq^{*1} \frac{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}||\tilde{\mathbf{y}} - \mathbf{y}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} = \frac{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}||\mathbf{y}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} \frac{|{\tilde{y}} - {y}|}{|{y}|} = \kappa_3 \frac{|{\tilde{y}} - {y}|}{|{y}|}$
mit $\kappa_3$ als Konditionszahl für (3)

$\frac{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y} - {\Sigma_{2,1}}\Sigma_{1,1}^{-1}\tilde{\mathbf{y}}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} \leq^{*1} \frac{|{\Sigma_{2,1}}||\tilde\Sigma_{1,1}^{-1}-\Sigma_{1,1}^{-1}||\mathbf{y}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} \leq^{*2} \frac{|\Sigma_{1,1}^{-1}|^{2}|{\Sigma_{2,1}}||\mathbf{y}||\Sigma_{1,1}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} \frac{|\tilde\Sigma_{1,1}^{-1} - \Sigma_{1,1}^{-1}|}{|\Sigma_{1,1}|} \leq |\Sigma_{1,1}||\Sigma_{1,1}^{-1}| \frac{|\Sigma_{1,1}^{-1}||{\Sigma_{2,1}}||\mathbf{y}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|} \frac{\tilde\Sigma_{1,1}^{-1} - \Sigma_{1,1}^{-1}}{\Sigma_{1,1}^{-1}} = \kappa_2\frac{\tilde\Sigma_{1,1}^{-1} - \Sigma_{1,1}^{-1}}{\Sigma_{1,1}^{-1}}$
mit $\kappa_2$ Konditionierungszahl für (2) als Produkt der
Konditionszahl $|\Sigma_{1,1}||\Sigma_{1,1}^{-1}|$ für die Berechnung
der Inversen von $\Sigma_{1,1}$ und der Konditionszahl
$\frac{|\Sigma_{1,1}^{-1}||{\Sigma_{2,1}}||\mathbf{y}|}{|{\Sigma_{2,1}}\Sigma_{1,1}^{-1}\mathbf{y}|}$
für die Matrixmultiplikation.

Die allgemeine Schranke für den (relativen) Outputfehler bezüglich aller
relativen Inputfehler ergibt sich aus der Summe der einzelnen Produkte
von Konditionszahl $\kappa_{\{1,2,3\}}$ und jeweiligem Fehler.
(BEGRÜNDUNG)

$*^1$ Submultiplikativität

$*^2$ Folgt aus Aufgabe 2.1c) der Übung inkl. Beweis zur Stetigkeit der
Matrixinvertierung in der Lösung

Theorem 1.1

Sei $A \in \mathbb{R^{n{\times}n}}$ invertierbar. Sei außerdem $|\cdot|$
eine zulässige Norm. Dann gilt für die (relative) Kondition:

$\kappa(A) = ||A||||A^{-1}||$, wobei die Kondition von der gewählten
Matrixnorm abhängt.

## 3.2 Einfluss der Hyperparameter auf die Konditionierung

Da die Konditionszahl des Gesamtproblems von der Kondition der
Kernmatrizen für $\Sigma_{1,1}$ und $\Sigma_{2,1}$ abhängt und diese
wiederum von der Kernfunktion $\exp(\frac{-s^2}{\gamma})$ abhängig sind,
betrachten wir zunächst den Einfluss von $\gamma$ auf die Konditionszahl
der Kernfunktion. Diese ergibt sich aus der Ableitung der Kernfunktion
nach $\gamma$:

$\kappa_{abs} = |\frac{\delta exp(\frac{-s^2}{\gamma})}{\delta\gamma}| = |exp(\frac{-s^2}{\gamma})\frac{s^2}{\gamma^2}| = exp(\frac{-s^2}{\gamma})\frac{s^2}{\gamma^2}$

$\rightarrow \kappa_{rel} = \frac{|\gamma|}{|exp(\frac{-s^2}{\gamma})|}exp(\frac{-s^2}{\gamma})\frac{s^2}{\gamma^2} = \frac{\gamma s^2}{\gamma^2} = \frac{s^2}{\gamma}$

Folglich ist für Werte von $\gamma \approx 0$ die Konditionszahl der
Kernfunktion groß und das Problem damit schlecht konditioniert. Für
$\gamma \geq s^2$ ist die Konditionszahl $\leq 1$ und das Problem
folglich gut konditioniert.

Da der Hyperparameter $\sigma$ nur zur Berechnung von $\Sigma_{1,1}$
verwendet wird, betrachten wir uns die Konditionszahl dieser Matrix,
wobei $\sigma_{max}$ maximaler Singulärwert und $\sigma_{min}$ minimaler
Singulärwert ist:

$\kappa_{\Sigma_{1,1}} = \frac{\sigma_{max}}{\sigma_{min}} = \frac{\Sigma_{1,1}}{\sigma_{min}} =^{*} \frac{K_{t,t} + \sigma^2I}{\sigma^2} > 1$

Folglich ist das Problem für $\sigma^2 \approx 0$ schlecht und für
$K_{t,t} \approx 0$ gut konditioniert.

$^* \sigma_{min} = \sigma^2$ folgt aus Beobachtung.

# 4 Algorithmus zur Lösung des Problems

Um den Algorithmus zu schreiben, der $\mu_{2|1}$ berechnet und unser
Problem löst, erstellen wir zunächst die Kernelfunktion sowie die
Funktion für unsere Kernelmatrix.

```{r}
# Kernfun: 
# Kernfun ist die Kernelfunktion, die als Prior dient und die Kovarianz für jede einzelne Kombination von Messzeitpunkten modelliert 
# Input: 
# d: absolute Differenz s zwischen zwei Messzeitpunkten
# gamma: beliebig wählbar, > 0
# Output: modellierte Kovarianz
Kernfun <- function(d, gamma) {
  exp(-(d^2) / gamma)
}

# matK: 
# matK ist die Kernelmatrix
# Input: 
## t: Vektor der Länge n, der Messzeitpunkte enthält
## s: Vektor mit Länge m, der Messzeitpunkte enthält
## gamma: beliebig wählbar, Input für die Kernelfunktion
# Output: eine Matrix n x m mit den modellierten Kovarianzen aus der Kernelfunktion als Einträge
matK <-
  function(t, s, gamma) {
    Kernfun(abs(
      matrix(t, nrow = length(t), ncol = length(s)) - matrix(
        s,
        nrow = length(t),
        ncol = length(s),
        byrow = TRUE
      )
    ), gamma)
  }
```

## 4.1 Unter Verwendung der Inversen

```{r}
alg_Inv <- function(t.Strich,
                 gamma = 1,
                 sigma.quadrat = 1) {
  Sigma1.1 <-
    matK(messungen$t, messungen$t, gamma) + sigma.quadrat * diag(length(messungen$t))
  Sigma2.1 <- t(matK(messungen$t, t.Strich, gamma))
  y <- Sigma2.1 %*% solve(Sigma1.1) %*% messungen$y
  y
}
```

Dieser Algorithmus ist schlecht, weil er die Inverse einer Matrix
verwendet. Ein Algorithmus, der eine Inverse verwendet, ist im
Allgemeinen instabiler als ein Algorithmus ohne Inverse, da die Inverse
nicht immer eindeutig ist. Auch wenn es nur eine Inverse gibt, kann der
Algorithmus immernoch anfälliger sein, weil die Berechnung einer
Inversen in der Regel numerisch anspruchsvoller ist als die Berechnung
anderer Funktionen. Die Verwendung einer Inversen kann daher dazu
führen, dass kleine numerische Fehler, die während der Berechnungen
auftreten, sich verstärken und zu größeren Fehlern in den endgültigen
Ergebnissen führen. Insgesamt ist es daher im Allgemeinen ratsam,
Algorithmen zu vermeiden, die eine Inverse verwenden, wenn es möglich
ist. Deshalb schreiben wir im Folgenden einen alternativen Algorithmus,
der die Berechnung der Inversen mithilfe der Cholesky Zerlegung
vermeidet.

## 4.2 Unter Verwendung der Cholesky-Zerlegung

Bei $\Sigma_{1,1}$ handelt es sich um eine Kovarianzmatrix. Folglich ist
$\Sigma_{1,1}$ symmetrisch und positiv definit und damit eine
SPD-Matrix. Das erlaubt es uns die Cholesky-Zerlegung zu verwenden und
$\Sigma_{1,1}$ zu $\Sigma_{1,1} = LL^t$ zu zerlegen, wobei $L$ eine
untere Dreiecksmatrix ist. Nun können wir durch Vorwärts- und
Rückwärtssubstitution das LGS lösen. Für dieses Vorgehen verwenden wir
in R die drei Subroutinen chol, forwardsolve und backsolve, mithilfe
derer wir das Invertieren der Matrix $\Sigma_{1,1}$ vermeiden und so
erheblichen Rechen- und Speicheraufwand einsparen.

```{r}
# alg_CH
# berechnet mü2.1 mittels Cholesky Zerlegung
# Input: 
## t.Strich: Vektor der Länge m mit Messzeitpunkten
## sigma.quadrat > 0 mit default = 1
## gamma > 0 mit default = 1
# Output:
## Vektor mü2.1 der Länge m

alg_CH <- function(t.Strich, sigma.quadrat = 1, gamma = 1){
  Sigma1.1 <-
    matK(messungen$t, messungen$t, gamma) + sigma.quadrat * diag(length(messungen$t))
  Sigma2.1 <- t(matK(messungen$t, t.Strich, gamma))
  L <- chol(Sigma1.1)
  B <- forwardsolve(t(L), messungen$y)
  Sigma2.1 %*% backsolve(L, B)
}
```

## 4.3 Unter Verwendung der Orthogonalen Diagonalisierung

Die Matrix Sigma1.1 ist symmetrisch. Folglich können wir sie orthogonal
diagonalisieren und damit die Berechnung der Inversen vermeiden, da für
orthogonale Matrizen gilt: solve(A) = t(A) und die Inverse einer
Diagonalmatrix sich einfach ergibt, indem man die Kehrbrüche auf der
Diagonalen berechnet.

```{r}
# alg_OD
# Berechnet mü2.1 mithilfe orthogonaler Diagonalisierung
# Input: 
## t.Strich: Vektor der Länge m mit Messzeitpunkten
## sigma.quadrat > 0 mit default = 1
## gamma > 0 mit default = 1
# Output:
## Vektor mü2.1 der Länge m

alg_OD <- function(t.Strich, sigma.quadrat = 1, gamma = 1){
  Sigma1.1 <-
    matK(messungen$t, messungen$t, gamma) + sigma.quadrat * diag(length(messungen$t))
  Sigma2.1 <- t(matK(messungen$t, t.Strich, gamma))
  P <- GramSchmidt(eigen(Sigma1.1)$vectors)  #Orthogonalisierung und Normalisierung mithilfe Gram Schmidt Algorithmus aus matlib package
  D.inv <- diag(1/eigen(Sigma1.1)$values)  
  y <- Sigma2.1 %*% t(P) %*% D.inv %*% P %*% messungen$y
  y
}
```

## 4.3 Laufzeit-Komplexität des Algorithmus unter Verwendung der Cholesky-Zerlegung

Die Laufzeit-Komplexität des Algorithmus, der die Cholesky-Zerlegung
verwendet, beträgt $\mathcal{O}(mn + n^3)$, wobei m und n durch die
Größe der Input-Daten bestimmt werden. Die Laufzeit-Komplexität ergibt
sich aus den folgenden Teil-Laufzeit-Komplexitäten:

-   Erstellen der Matrix $\Sigma_{2,1}$: $\mathcal{O}(mn)$

    Subtraktion jedes Eintrags des Vektors $\mathbf{t^`}$ mit Länge m
    von jedem Eintrag des Vektors $\mathbf{s}$ mit Länge n und
    anschließende Rechenoperationen (Multiplikation, Division) auf jedem
    der m\*n Einträge, die aber keine höhere Laufzeit-Komplexität als
    $\mathcal{O}(mn)$ haben.

-   Erstellen der Matrix $\Sigma_{1,1}$: $\mathcal{O}(n^2)$

    Subtraktion jedes Eintrags des Vektors $\mathbf{t}$ mit Länge n von
    jedem Eintrag desselben Vektors und anschließende Rechenoperationen
    (Multiplikation, Division) auf jedem der $n^2$ Einträge, die aber
    keine höhere Laufzeit-Komplexität als $\mathcal{O}(mn)$ haben.

-   Cholesky-Zerlegung: $\Sigma_{1,1}$: $\mathcal{O}(n^3)$

-   Vorwärts- und Rückwärtssubstitution: $\mathcal{O}(n^2)$

-   Matrix-Vektor-Multiplikation: $\mathcal{O}(mn)$

```{r}
grid <- expand.grid(
  m = seq.int(10, 100000, length = 4),
  n = seq.int(10, 100000, length = 4)
)

times1 <- numeric(nrow(grid))
times2 <- numeric(nrow(grid))

# for (k in 1:nrow(grid)) {
# m <- grid[k, 1]
# n <- grid[k, 2]
# times1[k] <- median(microbenchmark(alg_CH(m, sigma.quadrat = 1, gamma = 1), times = 10)$time)
# times2[k] <- median(microbenchmark(alg_Inv(m, sigma.quadrat = 1, gamma = 1), times = 10)$time)
# }
```

```{r}
cbind(grid, time = times1) |>
ggplot(aes(m, time, color = as.factor(n))) +
geom_line()
```

```{r}
cbind(grid, time = times2) |>
ggplot(aes(n, time, color = as.factor(m))) +
geom_line()
```

# 5 Implementierung des Algorithmus unter Verwendung der Cholesky-Zerlegung

Die verrauschten Messdaten des Physikers sind im Datensatz `messungen`
mit den Spalten t und y enthalten und sehen folgendermaßen aus:

```{r}
plot(messungen$t, messungen$y, xlab = "Messzeitpunkte t", ylab = "Messungen y", main = "Verrauschte Daten")
```

## 5.1 Rekonstruktion der unverrauschten Daten

```{r}
#Definiere Länge t.Strich
set.seed(5)
t.Strich <- sample(seq(from = 0, to = 5, length.out = 100), 100)
```

```{r}
curve(alg_CH(x, sigma.quadrat = 1, gamma = 1), from = 0, to = 5, col = "green", ylim = c(0, max(messungen$y, alg_CH(t.Strich))), ylab = "y", xlab = "t.Strich")
curve(alg_Inv(x, sigma.quadrat = 1, gamma = 1), from = 0, to = 5, col = "red", ylim = c(0, max(messungen$y, alg_CH(t.Strich))), ylab = "y", xlab = "t.Strich", add = TRUE)
curve(alg_OD(x, sigma.quadrat = 1, gamma = 1), from = 0, to = 5, col = "red", ylim = c(0, max(messungen$y, alg_CH(t.Strich))), ylab = "y", xlab = "t.Strich", add = TRUE)
points(messungen$t, messungen$y)



```

## 5.2 Einfluss der Hyperparameter und Numerisches Versagen

```{r}
# Define the function to compute MSE
mse <- function(gamma) {
  # Compute MSE for a given sigma.quadrat value
  mean(messungen$y - alg_CH(messungen$t, sigma.quadrat = 1, gamma))^2 + var(messungen$y - alg_CH(messungen$t, sigma.quadrat = 1, gamma))^2
}

# Generate a sequence of sigma.quadrat values
gammas <- seq(from = 0.0001, to = 100, by = 100)

# Compute the MSE for each sigma.quadrat value
mse_values <- mapply(mse, gammas)

# Plot the MSE values using the curve function
plot(mse_values, xlim=c(-3, 100), ylim=c(-1, 120), col="red", xlab="Gamma", ylab="MSE", type = "l")
```

# Fazit

Kondition, Stabilität, Effizienzbetrachtung

Betragsstriche

sigma.quadrat.quadrat / sigma.quadrat

Im Folgenden werden wir uns deshalb Zerlegungen von sigma.quadrat1.1
zunutze machen, um y ohne Inverse zu berechnen.

code verstecken

Um den Einfluss der Hyperparameter $\gamma$ und $\sigma.quadrat$ näher
zu untersuchen, visualisieren wir zunächst deren Einfluss auf die
Konditionszahl von $\sigma.quadrat_{1,1}$ für festes $\sigma = 1$ bzw.
festes $\gamma = 1$.

```{r}
# sigma.quadrat <- 1
# 
# kappa.fun1 <- function(gamma) {
#   Kernfun <- function(s) {
#     exp(-(s ^ 2) / gamma)
#   }
#   matK <-
#     function(t, s) {
#       Kernfun(abs(
#         matrix(t, nrow = length(t), ncol = length(s)) - matrix(
#           s,
#           nrow = length(t),
#           ncol = length(s),
#           byrow = TRUE
#         )
#       ))
#     }
#   Sigma1.1 <-
#     matK(messungen$t, messungen$t) + sigma.quadrat * diag(length(messungen$t))
#   return(max(svd(Sigma1.1)$d) / min(svd(Sigma1.1)$d))
# }
# 
# plot(gamma, sapply(gamma, kappa.fun1), type = "l", xlab = "gamma", ylab = "Konditionszahl")
# 
# 
# 
# gamma <- seq(0.1, 10, by = .1)

```

```{# {r}
# gamma <- 1
# 
# kappa.fun2 <- function(sigma.quadrat) {
#   Kernfun <- function(s) {
#     exp(-(s ^ 2) / gamma)
#   }
#   matK <-
#     function(t, s) {
#       Kernfun(abs(
#         matrix(t, nrow = length(t), ncol = length(s)) - matrix(
#           s,
#           nrow = length(t),
#           ncol = length(s),
#           byrow = TRUE
#         )
#       ))
#     }
#   Sigma1.1 <-
#     matK(messungen$t, messungen$t) + sigma.quadrat * diag(length(messungen$t))
#   return(max(svd(Sigma1.1)$d) / min(svd(Sigma1.1)$d))
# }
# 
# plot(sigma.quadrat, sapply(sigma.quadrat, kappa.fun2), type = "l", xlab = "sigma", ylab = "Konditionszahl")
# 
# 
# sigma.quadrat <- seq(0.1, 10, by = .1)
```

Wie verändert sich die Laufzeitkomplexität für die Längen 1, 10, 100,
1000 von t.Strich?

```{r}
t.Strich.1 <- seq(from = 0, to = 5, l = 1)

t.Strich.10 <- seq(from = 0, to = 5, l = 10)

t.Strich.100 <- seq(from = 0, to = 5, l = 100)

t.Strich.1000 <- seq(from = 0, to = 5, l = 1000)
```

Der Algorithmus, der die Cholesky-Zerlegung verwendet, kann in die
folgenden größeren Operationen aufgeteilt werden:
